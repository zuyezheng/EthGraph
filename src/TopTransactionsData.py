from multiprocessing import Pool

import pandas
from pandas import DataFrame

from functools import partial

from src.PricesData import PricesData
from src.TimeSpan import TimeSpan


def standardize_span(transactions_summary: DataFrame, span: TimeSpan, prices: PricesData, df: DataFrame):
    """ Standardize a dataframe from a single time span, static so it can be parallelized. """
    # ordered transactions by top from addresses
    top_from = df.groupby('from') \
        .agg({'value': ['sum', 'count']}) \
        .sort_values(by=[('value', 'sum')], ascending=False)

    # ordered transactions by top to addresses removing the from's we've already seen
    top_to = df.groupby('to') \
        .agg({'value': ['sum', 'count']}) \
        .sort_values(by=[('value', 'sum')], ascending=False)
    top_to = top_to.drop(top_from.index, errors='ignore')

    # merge the addresses
    addresses = pandas.concat([top_from, top_to])
    num_addresses = len(addresses.index)

    # add some standardized columns
    for metric in ['value', 'gas', 'gas_price']:
        df[metric + '_n'] = (df[metric] - transactions_summary[metric]['mean']) / transactions_summary[metric]['std']

    # encode addresses between 0 and 1 based on order
    def get_address_loc(address):
        if pandas.isna(address):
            return 0
        else:
            return (addresses.index.get_loc(address) + 1) / num_addresses

    for field in ['from', 'to']:
        df[field] = df.apply(lambda r: get_address_loc(r[field]), axis=1)

    # standardize the timestamp to get a temporal distribution vs just relative order by transaction size
    start_time = df.iloc[0]['time_span'] * span.seconds
    df['timestamp_s'] = ((df['timestamp'] - start_time) / (span.seconds * 0.5)) - 1

    # add the standardize price for the current time
    df['price_s'] = df.apply(lambda r: prices.get_price(r['timestamp'], True), axis=1)

    return df


class TopTransactionsData:
    """
    Load top transactions generated by EthTransactions and further aggregates and process them.

    @author zuye.zheng
    """

    def __init__(self,
        loc: str,
        span: TimeSpan,
        min_time_step: int = None,
        max_time_step: int = None
    ):
        """ Init with the directory location of the parquet files for transactions. """
        self.span = span

        # load the transactions and do any necessary clipping
        self.transactions = pandas.read_parquet(loc)
        if min_time_step is not None:
            self.transactions = self.transactions[self.transactions['time_span'] >= min_time_step]
        if max_time_step is not None:
            self.transactions = self.transactions[self.transactions['time_span'] <= max_time_step]

        # keep the columns we care about
        self.transactions = self.transactions[[
            'from_address', 'to_address', 'value_d', 'gas_d', 'gas_price', 'time_span', 'rank', 'block_timestamp'
        ]]

        # rename some columns
        self.transactions = self.transactions.rename(columns={
            # had to explicitly type convert some columns in spark, simplify the names
            'value_d': 'value',
            'gas_d': 'gas',
            # rename some other stuff to be more concise
            'from_address': 'from',
            'to_address': 'to',
            'block_timestamp': 'timestamp'
        }, errors="raise")

        # need to cast some types
        self.transactions['gas_price'] = self.transactions['gas_price'].astype(float)
        self.transactions['timestamp'] = self.transactions['timestamp'].astype(float)

        # do some aggregation
        self.transactions_agg = self.transactions.groupby('time_span').agg({
            'value': ['sum', 'mean', 'max', 'min', 'first', 'last', 'std', 'count'],
            'gas': ['sum', 'mean', 'max', 'min', 'first', 'last', 'std'],
            'gas_price': ['sum', 'mean', 'max', 'min', 'first', 'last', 'std']
        })
        self.transactions_summary = self.transactions.agg({
            'value': ['mean', 'max', 'min', 'first', 'last', 'std', 'count'],
            'gas': ['mean', 'max', 'min', 'first', 'last', 'std'],
            'gas_price': ['mean', 'max', 'min', 'first', 'last', 'std']
        })

    def standardize_transactions(self, prices: PricesData, concurrency: int = 48):
        """ Standardize all transactions with a given level of concurrency to speed things up. """
        with Pool(concurrency) as pool:
            standardized = pool.map(
                partial(standardize_span, self.transactions_summary, self.span, prices),
                self.chunk_transactions()
            )

        return pandas.concat(standardized)

    def chunk_transactions(self):
        """ Return an array of dataframes chunked by time span. """
        return [g[1] for g in self.transactions.groupby('time_span')]
